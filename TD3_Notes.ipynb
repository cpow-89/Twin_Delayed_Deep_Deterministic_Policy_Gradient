{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradient (TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- extended actor/critic method for continues action and input space\n",
    "- off policy model\n",
    "- the model uses one actor-network to calculate the actions given the states \n",
    "- the model uses two critics-networks(the twins) to calculate the q - Value given the actions and states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Set up:\n",
    "\n",
    "Actor:\n",
    "- build one actor-network and one actor target network(both share the same architecture)\n",
    "\n",
    "![ActorModel](img/actor_model.jpg)\n",
    "\n",
    "Critic Twins:\n",
    "- build two critic networks(the twins) and two critic target networks(all sharing the same architecture)\n",
    "\n",
    "![CriticModel](img/critic_model.jpg)\n",
    "\n",
    "Full Model:\n",
    "- in total the full model consists of 6 neural networks\n",
    "\n",
    "![FullModel](img/full_model.jpg)\n",
    "\n",
    "Replay Buffer:\n",
    "- initialize an experience replay buffer(we need a memory buffer cause TD3 is an off-policy model)\n",
    "- the buffer will be storing the last n transitions\n",
    "- transition consists of state, next state, action, reward and done information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "Exploration Phase:\n",
    "- fill the replay buffer with n transitions based on random actions\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "Step 1:\n",
    "- sample a batch of transitions(state($s$), next state($s'$), action($a$), reward($r$) and done information)\n",
    "\n",
    "For each transition in batch\n",
    "\n",
    "Step 2:\n",
    "- use next state($s'$) to calculate the next action($a'$) using the actor target network\n",
    "$a' = \\pi_{\\phi'}(s')$\n",
    "\n",
    "Step 3:\n",
    "- add noise($\\epsilon$) from a gaussian distribution($\\mathcal{N}$) to the next action($a'$) calculated in step 2(this leads to \"exploration\")\n",
    "\n",
    "$a' \\leftarrow a' + \\epsilon$\n",
    "\n",
    "$\\epsilon = clip(\\mathcal{N}(0, \\tilde\\sigma), -c, c) $\n",
    "\n",
    "- clamp the updated noisy next action($a'$) in the range min/max action value range supported by the environment\n",
    "\n",
    "$a' \\leftarrow clip(a')$\n",
    "\n",
    "Step 4:\n",
    "- use next state($s'$) and our calculated and updated next action($a'$) as input for the two critic target models to calculate two q-values $Q_1(s', a')$ and $Q_2(s', a')$ \n",
    "\n",
    "Step 5:\n",
    "- calculate the minimum of $Q_1(s', a')$ and $Q_2(s', a')$\n",
    "- Qmin is an approximation of the value of the next state\n",
    "- by calculating the minimum, we prevent to optimistic estimates which is a common problem in previous models\n",
    "\n",
    "$Qmin = min(Q_1(s', a'), Q_2(s', a'))$\n",
    "\n",
    "Step 2 till 5 visualized:\n",
    "\n",
    "![FullModel](img/forward_pass.jpg)\n",
    "\n",
    "Step 6:\n",
    "- calculate the target q value for the two critic models based on $Q_min$\n",
    "\n",
    "$Q_t = r + \\gamma * Q_min$\n",
    "\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "Step 7:\n",
    "- use state($s$) and the action($a$) as input for the two critic models to calculate two q-values $Q_1(s, a)$ and $Q_2(s, a)$ \n",
    "\n",
    "Step 8:\n",
    "- calculate the loss for the two critic models based on $Q_1(s, a)$, $Q_2(s, a)$ and $Q_t$\n",
    "\n",
    "$Loss_{Critic2} = MSELoss(Q_1, Q_t)$\n",
    "\n",
    "$Loss_{Critic2} = MSELoss(Q_2, Q_t)$\n",
    "\n",
    "Step 9:\n",
    "- update the weights of the two critic models based on $Loss_{Critic}$ using backpropagation \n",
    "\n",
    "For every n iterations(this is where the delayed in TD3 comes from):\n",
    "\n",
    "Step 10:\n",
    "- update the weights of the actor model by performing gradient ascent on the output of the first critic model\n",
    "- we basically update the actor parameters in the way that maximizes the q values which maximizes the expected return\n",
    "\n",
    "$\\triangledown_\\phi J(\\phi) = N^{-1}\\sum\\triangledown_aQ_{\\theta_1}(s,a)|_{a=\\pi_\\phi(s)} \\triangledown_\\phi \\pi_\\phi(s)$\n",
    "\n",
    "$\\phi$ = Actor weights, $\\theta_1$ = Critic weights\n",
    "\n",
    "Step 11:\n",
    "- smoothly update weights of the actor target using polyak averaging\n",
    "\n",
    "$\\theta' \\leftarrow \\tau\\theta + (1-\\tau) * \\theta'$ \n",
    "\n",
    "Step 12:\n",
    "- smoothly update weights of the two critic targets using polyak averaging\n",
    "\n",
    "$\\phi' \\leftarrow \\tau\\phi + (1-\\tau) * \\phi'$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
